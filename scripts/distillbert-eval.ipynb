{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":359162,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":299104,"modelId":319695}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T21:04:50.487886Z","iopub.execute_input":"2025-04-26T21:04:50.488227Z","iopub.status.idle":"2025-04-26T21:05:08.064659Z","shell.execute_reply.started":"2025-04-26T21:04:50.488195Z","shell.execute_reply":"2025-04-26T21:05:08.063612Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\nfrom datasets import load_dataset\n\nmodel_path = \"/kaggle/input/distillbert_trained_on_marco/transformers/default/1/trained_biencoder_distilbert_better/encoder\"\ntokenizer_path = \"/kaggle/input/distillbert_trained_on_marco/transformers/default/1/trained_biencoder_distilbert_better/tokenizer\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 64\nmax_length = 256\ntop_k = 5\n\n\nmodel = AutoModel.from_pretrained(model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\nmodel.eval()\n\n\ndef embed_texts(texts, batch_size=64):\n    embeddings = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n            batch_texts = texts[i:i+batch_size]\n            inputs = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n            outputs = model(**inputs).last_hidden_state.mean(dim=1)\n            embeddings.append(outputs.cpu())\n    embeddings = torch.cat(embeddings, dim=0)\n    return embeddings\n\n\nprint(\"Loading Natural Questions...\")\nnq = load_dataset(\"nq_open\", split=\"validation\")  \n\n\nqueries = nq['question']\nanswers = nq['answer']\npassages = []\nfor ans_list in answers:\n    if isinstance(ans_list, list) and len(ans_list) > 0:\n        passages.append(ans_list[0]) \n    else:\n        passages.append(\"\") \n\nprint(f\"Loaded {len(queries)} queries and passages.\")\nprint(\"Embedding passages...\")\npassage_embeddings = embed_texts(passages)\nprint(\"Building retrieval index...\")\nindex = NearestNeighbors(n_neighbors=top_k, metric=\"cosine\")\nindex.fit(passage_embeddings)\nprint(\"Embedding queries...\")\nquery_embeddings = embed_texts(queries)\nprint(\"Retrieving top-k...\")\ndistances, indices = index.kneighbors(query_embeddings, return_distance=True)\nrecall_at_1 = 0\nrecall_at_5 = 0\ntotal = len(queries)\n\nfor query_idx in range(total):\n    retrieved_indices = indices[query_idx]\n    correct_idx = query_idx  # because passage[i] corresponds to query[i] directly here\n    \n    if correct_idx == retrieved_indices[0]:\n        recall_at_1 += 1\n    if correct_idx in retrieved_indices:\n        recall_at_5 += 1\n\nrecall_at_1 /= total\nrecall_at_5 /= total\n\nprint(f\"\\n--- Retrieval Results on Natural Questions ---\")\nprint(f\"Recall@1: {recall_at_1:.4f}\")\nprint(f\"Recall@5: {recall_at_5:.4f}\")\n\n# Show 5 examples\nprint(\"\\nShowing first 5 queries and retrievals:\\n\")\nfor i in range(5):\n    print(f\"Query {i+1}: {queries[i]}\\n\")\n    for rank, idx in enumerate(indices[i]):\n        print(f\"Top {rank+1} passage: {passages[idx][:300]}...\")  # Show first 300 chars\n    print(\"-\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T21:13:12.576397Z","iopub.execute_input":"2025-04-26T21:13:12.576589Z","iopub.status.idle":"2025-04-26T21:14:42.845927Z","shell.execute_reply.started":"2025-04-26T21:13:12.576572Z","shell.execute_reply":"2025-04-26T21:14:42.845204Z"}},"outputs":[{"name":"stderr","text":"2025-04-26 21:13:28.896320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745702009.047274      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745702009.091457      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading Natural Questions...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260227953a1445a98864a822b6f77f98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/4.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d60a6b357c94fa482959e51da43a1f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/214k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf01deba010a4567865137883b8e291b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87925 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cbb1083c6ea472aa94a4fa9fb1a292f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3610 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe9bda671664e108c8360bbc1e3e905"}},"metadata":{}},{"name":"stdout","text":"Loaded 3610 queries and passages.\nEmbedding passages...\n","output_type":"stream"},{"name":"stderr","text":"Embedding: 100%|██████████| 57/57 [00:24<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Building retrieval index...\nEmbedding queries...\n","output_type":"stream"},{"name":"stderr","text":"Embedding: 100%|██████████| 57/57 [00:26<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Retrieving top-k...\n\n--- Retrieval Results on Natural Questions ---\nRecall@1: 0.0693\nRecall@5: 0.1452\n\nShowing first 5 queries and retrievals:\n\nQuery 1: when was the last time anyone was on the moon\n\nTop 1 passage: During the last Ice Age...\nTop 2 passage: last book...\nTop 3 passage: the following day...\nTop 4 passage: in the winter...\nTop 5 passage: the year of the Dog...\n--------------------------------------------------------------------------------\nQuery 2: who wrote he ain't heavy he's my brother lyrics\n\nTop 1 passage: I Write Sins Not Tragedies...\nTop 2 passage: his brother...\nTop 3 passage: Rich Mullins...\nTop 4 passage: her husband Albert Brown...\nTop 5 passage: A'ja Wilson...\n--------------------------------------------------------------------------------\nQuery 3: how many seasons of the bastard executioner are there\n\nTop 1 passage: \"Killer Within\"...\nTop 2 passage: Later in the sixth season...\nTop 3 passage: season two...\nTop 4 passage: the Four Seasons...\nTop 5 passage: in season two...\n--------------------------------------------------------------------------------\nQuery 4: when did the eagles win last super bowl\n\nTop 1 passage: just after the Super Bowl...\nTop 2 passage: in Super Bowl LII...\nTop 3 passage: American rock band Eagles...\nTop 4 passage: Super Bowl LII,...\nTop 5 passage: Super Bowl LII...\n--------------------------------------------------------------------------------\nQuery 5: who won last year's ncaa women's basketball\n\nTop 1 passage: Runner-up...\nTop 2 passage: Will Champion...\nTop 3 passage: won...\nTop 4 passage: in 1997...\nTop 5 passage: each team...\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nimport pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom sklearn.neighbors import NearestNeighbors\nimport numpy as np\n\nmodel_path = \"/kaggle/input/distillbert_trained_on_marco/transformers/default/1/trained_biencoder_distilbert_better/encoder\"\ntokenizer_path = \"/kaggle/input/distillbert_trained_on_marco/transformers/default/1/trained_biencoder_distilbert_better/tokenizer\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 64\nmax_length = 256\ntop_k = 5\n\nmodel = AutoModel.from_pretrained(model_path).to(device)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\nmodel.eval()\n\ndef embed_texts(texts, batch_size=64):\n    embeddings = []\n    with torch.no_grad():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n            batch_texts = texts[i:i+batch_size]\n            inputs = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n            outputs = model(**inputs).last_hidden_state.mean(dim=1)\n            embeddings.append(outputs.cpu())\n    embeddings = torch.cat(embeddings, dim=0)\n    return embeddings\n\nprint(\"Loading HotpotQA validation set...\")\nhotpot_val = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\")\n\nqueries = []\ngold_titles = [] \npassage_texts = []\npassage_titles = []\n\nfor example in tqdm(hotpot_val, desc=\"Processing Validation Data\"):\n    query = example['question']\n    queries.append(query)\n\n    support_titles = example['supporting_facts']['title']\n    gold_titles.append(set(support_titles))\n\n    for title, sentences_list in zip(example['context']['title'], example['context']['sentences']):\n        paragraph = \" \".join(sentences_list)\n        paragraph = paragraph.strip().replace(\"\\n\", \" \")\n        if len(paragraph.split()) >= 5:\n            passage_titles.append(title)\n            passage_texts.append(paragraph)\n\n\nprint(f\"Loaded {len(queries)} queries and {len(passage_texts)} passages.\")\nprint(\"Embedding passages...\")\npassage_embeddings = embed_texts(passage_texts)\nprint(\"Building retrieval index...\")\nindex = NearestNeighbors(n_neighbors=top_k, metric=\"cosine\")\nindex.fit(passage_embeddings)\nprint(\"Embedding queries...\")\nquery_embeddings = embed_texts(queries)\nprint(\"Retrieving top-k passages...\")\ndistances, indices = index.kneighbors(query_embeddings, return_distance=True)\n\nrecall_at_1 = 0\nrecall_at_5 = 0\ntotal = len(queries)\n\nfor query_idx in range(total):\n    retrieved_indices = indices[query_idx]\n    gold_set = gold_titles[query_idx]\n    retrieved_title_list = [passage_titles[i] for i in retrieved_indices]\n\n    if retrieved_title_list[0] in gold_set:\n        recall_at_1 += 1\n    if any(title in gold_set for title in retrieved_title_list):\n        recall_at_5 += 1\n\nrecall_at_1 /= total\nrecall_at_5 /= total\n\nprint(f\"\\n--- HotpotQA Retrieval Results ---\")\nprint(f\"Recall@1: {recall_at_1:.4f}\")\nprint(f\"Recall@5: {recall_at_5:.4f}\")\n\n# Show examples\nprint(\"\\nShowing first 5 queries and their retrievals:\\n\")\nfor i in range(5):\n    print(f\"Query {i+1}: {queries[i]}\")\n    print(f\"Gold Titles: {list(gold_titles[i])}\")\n    print(\"Top retrieved titles:\")\n    for rank, idx in enumerate(indices[i]):\n        print(f\"Top {rank+1}: {passage_titles[idx]}\")\n    print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T22:18:05.888153Z","iopub.execute_input":"2025-04-26T22:18:05.888417Z","iopub.status.idle":"2025-04-26T22:30:33.527876Z","shell.execute_reply.started":"2025-04-26T22:18:05.888399Z","shell.execute_reply":"2025-04-26T22:30:33.527095Z"}},"outputs":[{"name":"stdout","text":"Loading HotpotQA validation set...\n","output_type":"stream"},{"name":"stderr","text":"Processing Validation Data: 100%|██████████| 7405/7405 [00:01<00:00, 4011.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded 7405 queries and 73642 passages.\nEmbedding passages...\n","output_type":"stream"},{"name":"stderr","text":"Embedding: 100%|██████████| 1151/1151 [11:06<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Building retrieval index...\nEmbedding queries...\n","output_type":"stream"},{"name":"stderr","text":"Embedding: 100%|██████████| 116/116 [01:06<00:00,  1.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Retrieving top-k passages...\n\n--- HotpotQA Retrieval Results ---\nRecall@1: 0.5263\nRecall@5: 0.6705\n\nShowing first 5 queries and their retrievals:\n\nQuery 1: Were Scott Derrickson and Ed Wood of the same nationality?\nGold Titles: ['Scott Derrickson', 'Ed Wood']\nTop retrieved titles:\nTop 1: Jan D'Alquen and Ron Eveslage\nTop 2: Craig Marshall\nTop 3: Greg Bautzer\nTop 4: Greg Bautzer\nTop 5: James Kerwin\n--------------------------------------------------------------------------------\nQuery 2: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?\nGold Titles: ['Kiss and Tell (1945 film)', 'Shirley Temple']\nTop retrieved titles:\nTop 1: Meet Corliss Archer (TV series)\nTop 2: Elizabeth Norment\nTop 3: Harper Marshall\nTop 4: Sarah Coburn\nTop 5: Lisa Howard (American actress)\n--------------------------------------------------------------------------------\nQuery 3: What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\nGold Titles: ['Animorphs', 'The Hork-Bajir Chronicles']\nTop retrieved titles:\nTop 1: A Wizard of Earthsea\nTop 2: Animorphs\nTop 3: Journey into Mystery\nTop 4: Selected Stories of Philip K. Dick\nTop 5: Civil War: Young Avengers/Runaways\n--------------------------------------------------------------------------------\nQuery 4: Are the Laleli Mosque and Esma Sultan Mansion located in the same neighborhood?\nGold Titles: ['Laleli Mosque', 'Esma Sultan Mansion']\nTop retrieved titles:\nTop 1: Laleli Mosque\nTop 2: Mihrimah Sultan Mosque (Edirnekapı)\nTop 3: Süleymaniye Mosque\nTop 4: Süleymaniye Mosque\nTop 5: Esma Sultan Mansion\n--------------------------------------------------------------------------------\nQuery 5: The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?\nGold Titles: ['Adriana Trigiani', 'Big Stone Gap (film)']\nTop retrieved titles:\nTop 1: Big Stone Gap (film)\nTop 2: The Only Living Boy in New York (film)\nTop 3: The Big Split\nTop 4: Big Fish &amp; Begonia\nTop 5: In the Company of Men\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":24}]}