{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abaaeced-cda3-4f34-8204-03e09651afde",
   "metadata": {},
   "source": [
    "# Training Reader Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185407c7-a672-47fd-8916-e4a24d16b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW  \n",
    "from evaluate import load as load_metric\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e30dfb6-5c63-4be9-b00a-8a95dea25f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.9\n",
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU model: Tesla T4\n",
      "Number of GPUs: 1\n",
      "Available GPU memory: 15.64 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254480be-1880-4fe9-a6c5-7e2c804322de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD dataset…\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading SQuAD dataset…\")\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5f3020-7a25-4855-9663-828284ff7955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "Example entry: {'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "print(squad)\n",
    "print(\"Example entry:\", squad[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d1ec88c-9dfa-4d3b-ae54-901683004f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model & tokenizer\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "#model = torch.compile(model)  # requires PyTorch 2+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41838053-e10d-497f-9f4e-f63dc08c5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing SQuAD examples\n",
    "max_length = 384   \n",
    "doc_stride = 128   \n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_idx]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "            token_start_index, token_end_index = None, None\n",
    "            sequence_ids = tokenized.sequence_ids(i)\n",
    "            context_start = sequence_ids.index(1)\n",
    "            context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)\n",
    "            for idx in range(context_start, context_end + 1):\n",
    "                if offsets[idx][0] <= start_char < offsets[idx][1]:\n",
    "                    token_start_index = idx\n",
    "                    break\n",
    "            for idx in range(context_end, context_start - 1, -1):\n",
    "                if offsets[idx][0] < end_char <= offsets[idx][1]:\n",
    "                    token_end_index = idx\n",
    "                    break\n",
    "            if token_start_index is None or token_end_index is None:\n",
    "                token_start_index = cls_index\n",
    "                token_end_index = cls_index\n",
    "\n",
    "            start_positions.append(token_start_index)\n",
    "            end_positions.append(token_end_index)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized\n",
    "\n",
    "# Using datasets.map to preprocess train and validation\n",
    "train_dataset = squad[\"train\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"train\"].column_names,\n",
    ")\n",
    "eval_dataset = squad[\"validation\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=squad[\"validation\"].column_names,\n",
    ")\n",
    "\n",
    "# Converting to PyTorch DataLoader\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        return {k: torch.tensor(v) for k, v in example.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b6fb57-8d86-4069-b18a-edefd2efaf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|████████████████████████████████████████████████████████| 2767/2767 [12:27<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 avg train loss: 0.7485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 quick val loss: 1.1257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|████████████████████████████████████████████████████████| 2767/2767 [12:25<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 avg train loss: 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 quick val loss: 1.1752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine Tuning\n",
    "\n",
    "# DataLoader \n",
    "train_loader = DataLoader(\n",
    "    QADataset(train_dataset),\n",
    "    batch_size=32,          \n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8,\n",
    "    persistent_workers=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    QADataset(eval_dataset),\n",
    "    batch_size=32,\n",
    "    pin_memory=True,\n",
    "    num_workers=8,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "epochs = 2\n",
    "learning_rate = 3e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop \n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids        = batch[\"input_ids\"].to(device)\n",
    "        attention_mask   = batch[\"attention_mask\"].to(device)\n",
    "        start_positions  = batch[\"start_positions\"].to(device)\n",
    "        end_positions    = batch[\"end_positions\"].to(device)\n",
    "\n",
    "        with autocast():  \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} avg train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Quick validation on first 20 batches\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in list(eval_loader)[:20]:\n",
    "            input_ids       = batch[\"input_ids\"].to(device)\n",
    "            attention_mask  = batch[\"attention_mask\"].to(device)\n",
    "            start_positions = batch[\"start_positions\"].to(device)\n",
    "            end_positions   = batch[\"end_positions\"].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "            val_loss += outputs.loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = val_loss / val_steps\n",
    "    print(f\"Epoch {epoch+1} quick val loss: {avg_val_loss:.4f}\\n\")\n",
    "\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf738-77a0-45b5-9c82-793ffcef62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 1. Save the model & tokenizer\n",
    "model.save_pretrained(\"checkpoint/reader_model\")\n",
    "tokenizer.save_pretrained(\"checkpoint/reader_tokenizer\")\n",
    "\n",
    "# 2. Save optimizer & scheduler state, plus current epoch\n",
    "import torch, os\n",
    "os.makedirs(\"checkpoint\", exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": epoch,                         \n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, \"checkpoint/reader_training.pt\")\n",
    "\n",
    "print(\"Checkpoint saved!\")\n",
    "\n",
    "# 1) Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2) Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoint/reader_tokenizer\")\n",
    "model     = AutoModelForQuestionAnswering.from_pretrained(\"checkpoint/reader_model\")\n",
    "model     = model.to(device)\n",
    "\n",
    "# 3) Create optimizer & scheduler with same settings\n",
    "learning_rate = 3e-5\n",
    "epochs        = 2        \n",
    "train_loader = DataLoader(\n",
    "    QADataset(train_dataset),\n",
    "    batch_size=16,            \n",
    "    shuffle=True,\n",
    "    pin_memory=True,          \n",
    "    num_workers=4             \n",
    ")  \n",
    "total_steps   = len(train_loader) * epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = int(0.1 * total_steps),\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "# 4) Load the training state\n",
    "ckpt = torch.load(\"checkpoint/reader_training.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "start_epoch = ckpt[\"epoch\"] + 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f469a-465d-46e4-bfc3-81166704f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''eval_loader = DataLoader(\n",
    "    QADataset(eval_dataset),\n",
    "    batch_size=8,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d250440-ad1f-400a-b3d8-62ec22876021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 75.43\n",
      "F1 Score    : 84.12\n",
      "\n",
      "Example #1:\n",
      "Question:         Which NFL team represented the AFC at Super Bowl 50?\n",
      "Context excerpt:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Gold Answers:     ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      "Model Prediction: Denver Broncos\n",
      "Example #2:\n",
      "Question:         Which NFL team represented the NFC at Super Bowl 50?\n",
      "Context excerpt:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Gold Answers:     ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers']\n",
      "Model Prediction: Carolina Panthers\n",
      "Example #3:\n",
      "Question:         Where did Super Bowl 50 take place?\n",
      "Context excerpt:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "Gold Answers:     ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      "Model Prediction: Levi's Stadium\n"
     ]
    }
   ],
   "source": [
    "# ──────────── Full Batched QA Evaluation────────────\n",
    "\n",
    "# QA pipeline \n",
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,        \n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    handle_impossible_answer=False,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Load Data\n",
    "raw_eval = load_dataset(\"squad\", split=\"validation\")\n",
    "questions = raw_eval[\"question\"]\n",
    "contexts  = raw_eval[\"context\"]\n",
    "\n",
    "# Runnig\n",
    "preds = qa_pipe(question=questions, context=contexts)\n",
    "predictions = [\n",
    "    {\"id\": ex[\"id\"], \"prediction_text\": out[\"answer\"]}\n",
    "    for ex, out in zip(raw_eval, preds)\n",
    "]\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
    "    for ex in raw_eval\n",
    "]\n",
    "\n",
    "metric  = load_metric(\"squad\")\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score    : {results['f1']:.2f}\\n\")\n",
    "\n",
    "# Example Prediction\n",
    "i = 0  # index of the example to display\n",
    "print(\"Example #1:\")\n",
    "print(\"Question:        \", questions[i])\n",
    "print(\"Context excerpt: \", contexts[i])\n",
    "print(\"Gold Answers:    \", raw_eval[i][\"answers\"][\"text\"])\n",
    "print(\"Model Prediction:\", preds[i][\"answer\"])\n",
    "\n",
    "i+=1\n",
    "print(\"Example #2:\")\n",
    "print(\"Question:        \", questions[i])\n",
    "print(\"Context excerpt: \", contexts[i])\n",
    "print(\"Gold Answers:    \", raw_eval[i][\"answers\"][\"text\"])\n",
    "print(\"Model Prediction:\", preds[i][\"answer\"])\n",
    "\n",
    "i+=1\n",
    "print(\"Example #3:\")\n",
    "print(\"Question:        \", questions[i])\n",
    "print(\"Context excerpt: \", contexts[i])\n",
    "print(\"Gold Answers:    \", raw_eval[i][\"answers\"][\"text\"])\n",
    "print(\"Model Prediction:\", preds[i][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5815d8bc-70d5-42ee-89e3-67d49d0a0ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & tokenizer saved to `final_qa_model/`\n"
     ]
    }
   ],
   "source": [
    "# Saving Model\n",
    "'''\n",
    "save_dir = \"final_qa_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Model & tokenizer saved to `{save_dir}/`\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87960296-32d8-45ae-a3c3-962db030fd9f",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c8415a-c424-401c-86a5-691862e06f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded and ready!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"final_qa_model\")\n",
    "model     = AutoModelForQuestionAnswering.from_pretrained(\"final_qa_model\")\n",
    "model     = model.to(device)\n",
    "print(\"Model reloaded and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b606f0-eab3-4947-8e6c-a6a64670bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 78.67\n",
      "F1 Score    : 86.55\n",
      "\n",
      "Example #1:\n",
      " Question:        Which NFL team represented the AFC at Super Bowl 50?\n",
      " Context excerpt: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated …\n",
      " Gold Answers:    ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\n",
      " Model Prediction: Denver Broncos\n",
      "--------------------------------------------------------------------------------\n",
      "Example #2:\n",
      " Question:        Which NFL team represented the NFC at Super Bowl 50?\n",
      " Context excerpt: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated …\n",
      " Gold Answers:    ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers']\n",
      " Model Prediction: Carolina Panthers\n",
      "--------------------------------------------------------------------------------\n",
      "Example #3:\n",
      " Question:        Where did Super Bowl 50 take place?\n",
      " Context excerpt: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated …\n",
      " Gold Answers:    ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\n",
      " Model Prediction: Levi's Stadium\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ensuring that model working properly\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from transformers import pipeline\n",
    "model.eval()\n",
    "\n",
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    handle_impossible_answer=False,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "raw_eval = load_dataset(\"squad\", split=\"validation\")\n",
    "questions = raw_eval[\"question\"]\n",
    "contexts  = raw_eval[\"context\"]\n",
    "\n",
    "preds = qa_pipe(question=questions, context=contexts)\n",
    "\n",
    "predictions = [\n",
    "    {\"id\": ex[\"id\"], \"prediction_text\": out[\"answer\"]}\n",
    "    for ex, out in zip(raw_eval, preds)\n",
    "]\n",
    "references = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]}\n",
    "    for ex in raw_eval\n",
    "]\n",
    "\n",
    "metric  = load_metric(\"squad\")\n",
    "results = metric.compute(predictions=predictions, references=references)\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score    : {results['f1']:.2f}\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Example #{i+1}:\")\n",
    "    print(\" Question:       \", questions[i])\n",
    "    print(\" Context excerpt:\", contexts[i][:200].replace(\"\\n\", \" \"), \"…\")\n",
    "    print(\" Gold Answers:   \", raw_eval[i][\"answers\"][\"text\"])\n",
    "    print(\" Model Prediction:\", preds[i][\"answer\"])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f9bdf-f741-4442-a7da-5826393c809a",
   "metadata": {},
   "source": [
    "# Utilizing entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2564d46f-7d7f-4a9e-9bc8-0006ca955a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d018cece-235f-45c6-821a-dc71f89a2b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever model and tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "# Loading retriever model\n",
    "encoder_path   = \"model2/encoder\"\n",
    "tokenizer_path = \"model2/tokenizer\"\n",
    "tokenizer_retriever = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "retrieve_model      = AutoModel.from_pretrained(encoder_path).to(device)\n",
    "retrieve_model.eval()\n",
    "print(\"Retriever model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35c68e65-d211-4328-9758-e48633f63d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/tgs2126/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to /home/tgs2126/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tgs2126/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/tgs2126/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO train set…\n",
      "Extracting and splitting passages…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MS MARCO items: 100%|█████████████████████████████████████████████████| 808731/808731 [03:13<00:00, 4184.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 99394 passages to cleaned_query_passage_pairs.parquet\n"
     ]
    }
   ],
   "source": [
    "# Rebuilding cleaned_query_passage_pairs.parquet (same code in used in retriever model)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_passages(text, min_tokens=80, max_tokens=300):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "    passages = []\n",
    "    current = \"\"\n",
    "    for sent in sentences:\n",
    "        if len(current.split()) + len(sent.split()) <= max_tokens:\n",
    "            current += \" \" + sent\n",
    "        else:\n",
    "            if len(current.split()) >= min_tokens:\n",
    "                passages.append(current.strip())\n",
    "            current = sent\n",
    "    if len(current.split()) >= min_tokens:\n",
    "        passages.append(current.strip())\n",
    "    return passages\n",
    "\n",
    "print(\"Loading MS MARCO train set…\")\n",
    "dataset = load_dataset(\"ms_marco\", \"v2.1\", split=\"train\")\n",
    "\n",
    "queries, passages = [], []\n",
    "print(\"Extracting and splitting passages…\")\n",
    "for item in tqdm(dataset, desc=\"MS MARCO items\"):\n",
    "    query = item.get(\"query\", \"\")\n",
    "    pinfo = item.get(\"passages\", {})\n",
    "    for is_sel, ptext in zip(pinfo.get(\"is_selected\", []), pinfo.get(\"passage_text\", [])):\n",
    "        if is_sel == 1:\n",
    "            txt = clean_text(ptext)\n",
    "            for chunk in split_passages(txt):\n",
    "                queries.append(query)\n",
    "                passages.append(chunk)\n",
    "'''\n",
    "df = pd.DataFrame({\"query\": queries, \"passage\": passages})\n",
    "df.to_parquet(\"cleaned_query_passage_pairs.parquet\", index=False)\n",
    "print(f\"Saved {len(passages)} passages to cleaned_query_passage_pairs.parquet\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d00679-5108-4056-8b4d-966f774993ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding passages: 100%|███████████████████████████████████████████████████| 1554/1554 [12:18<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages embedded and index built.\n"
     ]
    }
   ],
   "source": [
    "# 1) Loading data\n",
    "df_passages = pd.read_parquet(\"cleaned_query_passage_pairs.parquet\")\n",
    "passages = df_passages[\"passage\"].tolist()\n",
    "\n",
    "# 2) embedding function using our retriever model\n",
    "def embed_texts(texts, batch_size=64):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding passages\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "            inputs = tokenizer_retriever(\n",
    "                batch,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            outputs = retrieve_model(**inputs)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1).cpu()\n",
    "            embeddings.append(emb)\n",
    "    return torch.cat(embeddings, dim=0).numpy()\n",
    "\n",
    "# 3) We embed all passages\n",
    "passage_embeddings = embed_texts(passages)\n",
    "\n",
    "# 4) We build the FAISS index\n",
    "top_k = 5\n",
    "index = NearestNeighbors(n_neighbors=top_k, metric=\"cosine\")\n",
    "index.fit(passage_embeddings)\n",
    "print(\"Passages embedded and index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bef487-51be-4f66-9d24-d9ea9fd412aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the same embed_texts helper for queries\n",
    "def embed_queries(queries, batch_size=64):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(queries), batch_size), desc=\"Embedding queries\"):\n",
    "            batch = queries[i : i + batch_size]\n",
    "            inputs = tokenizer_retriever(\n",
    "                batch,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            outputs = retrieve_model(**inputs)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1).cpu()\n",
    "            embeddings.append(emb)\n",
    "    return torch.cat(embeddings, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a0ff3a-6aae-41f1-a9fe-c3a31b54291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding queries: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What causes rainbows to form?\n",
      "  Top 1 (score=0.643): A rainbow is a meteorological phenomenon that is caused by reflection, refraction and dispersion of light in water droplets resulting in a spectrum of light appearing in the sky. It takes the form of …\n",
      "  Top 2 (score=0.629): A rainbow is a meteorological phenomenon that is caused by reflection, refraction and dispersion of light in water droplets resulting in a spectrum of light appearing in the sky.It takes the form of a…\n",
      "  Top 3 (score=0.620): A rainbow is caused by the refraction and internal reflection of light inside rain drops, which results in the white sunlight being separated out into the colours of the rainbow. See the detailed page…\n",
      "  Top 4 (score=0.578): The longer wavelength coloured light, such as red, has a large rainbow angle, then the short wavelength colours, such as blue. The index of refraction of light in water is a measure of the speed of li…\n",
      "  Top 5 (score=0.575): Split and merge into it. Answer by Sedux. Confidence votes 120. In science what causes a revolution is the rotation of an object on its axis. In society, revolution is usually caused by injustices ren…\n",
      "\n",
      "Query: Who wrote the novel 1984?\n",
      "  Top 1 (score=0.498): It's called hate speech. If there ever were a more Orwellian concept, it would be difficult to find. For much like the concept of thought crimes in George Orwell 's novel 1984, hate crimes and hate sp…\n",
      "  Top 2 (score=0.497): For the 1986 film adaptation, see The Name of the Rose (film) . For other uses, see The Name of the Rose (disambiguation) . The Name of the Rose (Italian: Il nome della rosa) is the 1980 debut novel b…\n",
      "  Top 3 (score=0.496): The Hunger (1983 film) The Hunger is a 1983 British-American erotic horror film directed by Tony Scott, and starring Catherine Deneuve, David Bowie, and Susan Sarandon. It is the story of a love trian…\n",
      "  Top 4 (score=0.488): Orwell devoted his energy to writing novels that were politically charged, first with Animal Farm in 1945, then with 1984 in 1949. 1984 is one of Orwell s best-crafted novels, and it remains one of th…\n",
      "  Top 5 (score=0.479): Dirty Realism is the fiction of a new generation of American authors. They write about the belly-side of contemporary life a deserted husband, an unwed mother, a car thief, a pickpocket, a drug addict…\n",
      "\n",
      "Query: How do neural networks learn?\n",
      "  Top 1 (score=0.614): Your brain, spinal cord and peripheral nerves make up a complex, integrated information-processing and control system known as your central nervous system. In tandem, they regulate all the conscious a…\n",
      "  Top 2 (score=0.612): A neural pathway, neural tract, or neural face, connects one part of the nervous system with another via a bundle of axons, the long fibers of neurons.A neural pathway that serves to connect relativel…\n",
      "  Top 3 (score=0.612): A neural pathway, neural tract, or neural face, connects one part of the nervous system with another via a bundle of axons, the long fibers of neurons.A neural pathway that serves to connect relativel…\n",
      "  Top 4 (score=0.606): A network packet is a formatted unit of data carried by a packet-switched network. Computer communications links that do not support packets, such as traditional point-to-point telecommunications link…\n",
      "  Top 5 (score=0.605): What is STEM Literacy? STEM literacy is the ability to identify, apply, and integrate concepts from science, technology, engineering, and mathematics to understand complex problems and to innovate to …\n"
     ]
    }
   ],
   "source": [
    "# Queries of interest\n",
    "example_queries = [\n",
    "    \"What causes rainbows to form?\",\n",
    "    \"Who wrote the novel 1984?\",\n",
    "    \"How do neural networks learn?\"\n",
    "]\n",
    "\n",
    "# We embed them\n",
    "query_embeddings = embed_queries(example_queries)\n",
    "\n",
    "# We retrieve top-k passages per query\n",
    "distances, indices = index.kneighbors(query_embeddings, return_distance=True)\n",
    "for qi, query in enumerate(example_queries):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for rank, idx in enumerate(indices[qi]):\n",
    "        print(f\"  Top {rank+1} (score={1-distances[qi][rank]:.3f}): {passages[idx][:200]}…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf30f8-a139-46c5-bdac-ea19ed38d4ee",
   "metadata": {},
   "source": [
    "# Testing the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "615b62a3-68b5-41fb-adc3-d4f0c53144a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is the chemical formula for water?\",\n",
    "    \"How far is the Moon from Earth?\",\n",
    "    \"When did the first human walk on the Moon?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b9f9c35-a1dc-4412-9e8c-c13873a53619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding queries: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the capital of France?\n",
      "Answer: Paris (score=0.993)\n",
      "From passage: France spans 643,801 square kilometres (248,573 sq mi) and has a total population of 66.7 million. It is a unitary semi-presidential republic with the capital in Paris, the country's largest city and main cultural and commercial centre. During the Iron Age, what is now metropolitan France was inhabited by the Gauls, a Celtic people. The area was annexed in 51 BC by Rome, which held Gaul until 486, when the Germanic Franks conquered the region and formed the Kingdom of France. \n",
      "\n",
      "\n",
      "Query: What is the chemical formula for water?\n",
      "Answer: H 2 O (score=0.974)\n",
      "From passage: Answer: Yes, water is a compound. A compound forms whenever two or more atoms form chemical bonds with each other. The chemical formula for water is H 2 O, which means each molecule of water consists of one oxygen atom chemically bonded to two hydrogen atoms.Thus, water is a compound.It's also a molecule, which is any chemical species formed by two or more atoms chemically bonded to each other. The terms molecule and compound mean the same thing and can be used interchangeably.he chemical formula for water is H 2 O, which means each molecule of water consists of one oxygen atom chemically bonded to two hydrogen atoms. Thus, water is a compound. It's also a molecule, which is any chemical species formed by two or more atoms chemically bonded to each other. \n",
      "\n",
      "\n",
      "Query: How far is the Moon from Earth?\n",
      "Answer: 384, 000 km (score=0.948)\n",
      "From passage: As we all know, earth and moon form a part of our solar system. The surface area of the moon is 37.8 million square km and the surface area of the earth is 510 million square km. The moon is situated 384, 000 km away from the earth. The earth is situated 149, 668, 992 km (93, 000, 000 miles) from the sun. The earth s distance from the sun is conducive to life. Also, the moon does not have water, but the earth has water. \n",
      "\n",
      "\n",
      "Query: When did the first human walk on the Moon?\n",
      "Answer: July 20, 1969 (score=0.901)\n",
      "From passage: The first person to go to the moon was the Apollo 8 crew, which consisted of Frank Borman (Commander), Jim Lovell (Command Module Pilot), and Wiliam Anders (Lunar Module P ilot). Since there was no lunar module, Anders was tasked with taking photographs of the moon. It was he who shot the famous Earthrise photo from lunar orbit on Christmas Eve, 1968. The first person to land on the moon was Neil Armstrong and Buzz Aldrin of Apollo 11 on July 20, 1969. The first person to actually walk on the moon was Armstrong, who set foot on the surface about 6 hours after the landing. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Embeding\n",
    "query_embeddings = embed_queries(my_queries)\n",
    "\n",
    "# 2) Retrieving top-k passages\n",
    "distances, indices = index.kneighbors(query_embeddings, return_distance=True)\n",
    "\n",
    "# 3) Runing the reader on each query’s passages\n",
    "for qi, query in enumerate(my_queries):\n",
    "    candidate_passages = [passages[idx] for idx in indices[qi]]\n",
    "    qa_inputs = {\n",
    "        \"question\": [query] * top_k,\n",
    "        \"context\" : candidate_passages\n",
    "    }\n",
    "    outputs = qa_pipe(**qa_inputs)\n",
    "\n",
    "    best = max(outputs, key=lambda x: x[\"score\"])\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Answer: {best['answer']} (score={best['score']:.3f})\")\n",
    "    print(\"From passage:\", candidate_passages[outputs.index(best)], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b5b54-3df4-4166-a53e-d278933c5a28",
   "metadata": {},
   "source": [
    "# Stress-test for showing model vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c77380cc-73a3-413c-8825-fcb5ed14b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are only intended to demonstrate the model's vulnerability to more \n",
    "# difficult sequences and will help us understand how we can improve it in the future.\n",
    "my_queries = [\n",
    "    \"Who was the second person to walk on the Moon, and what city was he born in?\",\n",
    "    \"Which Nobel Prize in Literature laureate was born the same year the RMS Titanic sank?\",\n",
    "    \"When did she win her first Olympic gold?\",\n",
    "    \"How many countries share a land border with Germany?\",\n",
    "    \"Which chemical element has an atomic number equal to the number of letters in its English name?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbff220b-ead7-4942-a71d-f5c09f960140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding queries: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Who was the second person to walk on the Moon, and what city was he born in?\n",
      "Answer: Roald Amundsen (score=0.598)\n",
      "From passage: The Norwegian explorer Roald Amundsen was one of the most important people in the history of polar exploration. He traveled to both the Arctic and the Antarctic, and he was the first person to reach the South Pole. (1872-1928). One of the most important figures in the history of polar exploration was Roald Amundsen. He was the first person to reach the South Pole, the first to sail through the Northwest Passage, and the first to fly over the North Pole. \n",
      "\n",
      "\n",
      "Query: Which Nobel Prize in Literature laureate was born the same year the RMS Titanic sank?\n",
      "Answer: Jose Echegaray (score=0.849)\n",
      "From passage: The news that she had been awarded the Nobel Prize for Literature came when the Chilean poet was serving as a consul in the city of Petropolis, Brazil. In 1945, Gabriela Mistral became the first Latin American to ever win the Nobel Prize for Literature.owever, h received the Nobel Prize for his work as a playwright, having written 67 plays such as The Great Galeotti (1881). The choice to award the Literature Nobel Prize to Jose Echegaray was widely criticized by other Spanish writers such as Leopoldo Alas (Clarin) and Emilia Pardo Bazan. \n",
      "\n",
      "\n",
      "Query: When did she win her first Olympic gold?\n",
      "Answer: 1896 (score=0.908)\n",
      "From passage: The modern Olympics were first held in 1896 in Athens, Greece. They began after decades of increased interest in reviving the ancient games. Various Olympic-style games had been held starting in the 1600s, although they were small and mostly involved participants from the regions where they were held.enewed Interest. Interest in reviving the ancient games began to increase after the Greece's war of independence from the Ottoman Empire, which lasted from 1821 to 1832. In 1856, Evangelos Zappas, a wealthy Greek businessman, made an offer to Greece's King Otto to sponsor modern Olympic Games. \n",
      "\n",
      "\n",
      "Query: How many countries share a land border with Germany?\n",
      "Answer: Germany (score=0.791)\n",
      "From passage: Europe is divided from Asia on the east by the water divide of the Ural Mountains, the Ural River, the Caspian Sea and the Caucasus Mountains. The Black Sea to the southeast, the Atlantic Ocean in the west and the Arctic Ocean in the north. Europe is the second smallest continent by surface area. Western Russia is a part of the European continent. Europe consists of the countries of Finland, Sweden, Norway, Scotland, England, Iceland, Ireland, Portugal, Spain, France, Switzerland, Italy, Germany, Greece, Turkey, Poland, Denmark among others. Between the 16th and 20th centuries, European nations controlled at various times the Americas, most of Africa, Oceania and large portions of Asia. \n",
      "\n",
      "\n",
      "Query: Which chemical element has an atomic number equal to the number of letters in its English name?\n",
      "Answer: Z (score=0.811)\n",
      "From passage: A chemical element or element is a chemical substance consisting of atoms having the same number of protons in their atomic nuclei (i.e. the same atomic number, Z).There are 118 elements that have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements.sotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having different numbers of neutrons. Most (66 of 94) naturally occurring elements have more than one stable isotope. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Embeding\n",
    "query_embeddings = embed_queries(my_queries)\n",
    "\n",
    "# 2) Retrieving top-k passages\n",
    "distances, indices = index.kneighbors(query_embeddings, return_distance=True)\n",
    "\n",
    "# 3) Runing the reader on each query’s passages\n",
    "for qi, query in enumerate(my_queries):\n",
    "    candidate_passages = [passages[idx] for idx in indices[qi]]\n",
    "    qa_inputs = {\n",
    "        \"question\": [query] * top_k,\n",
    "        \"context\" : candidate_passages\n",
    "    }\n",
    "    outputs = qa_pipe(**qa_inputs)\n",
    "\n",
    "    best = max(outputs, key=lambda x: x[\"score\"])\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Answer: {best['answer']} (score={best['score']:.3f})\")\n",
    "    print(\"From passage:\", candidate_passages[outputs.index(best)], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c635df-0417-46b0-8c55-d505a799b882",
   "metadata": {},
   "source": [
    "# Final Pipeline Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7571ba-05c8-456f-b224-2675e8ad05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding passages: 100%|█████████████████████████████████████████████████████| 166/166 [01:21<00:00,  2.05it/s]\n",
      "Embedding queries: 100%|██████████████████████████████████████████████████████| 166/166 [01:17<00:00,  2.15it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Recall@5: 0.503\n",
      "Retrieval Recall@20: 0.742\n",
      "Retrieval MRR@20:    0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "End-to-end QA:   0%|                                                          | 6/10570 [00:00<06:26, 27.35it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "End-to-end QA: 100%|██████████████████████████████████████████████████████| 10570/10570 [05:45<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match: 44.50\n",
      "F1 Score    : 50.70\n"
     ]
    }
   ],
   "source": [
    "# 1) Device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 2) Retrieval settings\n",
    "recall_k1 = 5\n",
    "recall_k2 = 20\n",
    "\n",
    "# 3) Loading SQuAD validation\n",
    "squad    = load_dataset(\"squad\", split=\"validation\")\n",
    "questions = squad[\"question\"]\n",
    "contexts   = squad[\"context\"]\n",
    "answers    = squad[\"answers\"]\n",
    "n          = len(questions)\n",
    "\n",
    "# 4) Building retrieval index \n",
    "context_embeddings = embed_texts(contexts)  \n",
    "index = NearestNeighbors(n_neighbors=recall_k2, metric=\"cosine\")\n",
    "index.fit(context_embeddings)\n",
    "\n",
    "# 5) Retrieve top 20\n",
    "query_embeddings = embed_queries(questions)\n",
    "distances, indices = index.kneighbors(query_embeddings, return_distance=True)\n",
    "\n",
    "# 6) Recall@5 and Recall@20\n",
    "recall5  = np.mean([1 if i in idxs[:recall_k1] else 0 for i, idxs in enumerate(indices)])\n",
    "recall20 = np.mean([1 if i in idxs else 0 for i, idxs in enumerate(indices)])\n",
    "rr20 = [\n",
    "    1.0/(idxs.tolist().index(i)+1)  \n",
    "    if i in idxs else 0.0           \n",
    "    for i, idxs in enumerate(indices)\n",
    "]\n",
    "mrr20 = np.mean(rr20)\n",
    "\n",
    "print(f\"Retrieval Recall@{recall_k1}: {recall5:.3f}\")\n",
    "print(f\"Retrieval Recall@{recall_k2}: {recall20:.3f}\")\n",
    "print(f\"Retrieval MRR@{recall_k2}:    {mrr20:.3f}\")\n",
    "\n",
    "# 7) QA pipeline\n",
    "qa_pipe = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    handle_impossible_answer=False,\n",
    "    batch_size=recall_k2  \n",
    ")\n",
    "\n",
    "# 8) EM/F1\n",
    "predictions = []\n",
    "for i in tqdm(range(n), desc=\"End-to-end QA\"):\n",
    "    q     = questions[i]\n",
    "    cands = [contexts[j] for j in indices[i][:recall_k1]]\n",
    "    outs  = qa_pipe(question=[q]*recall_k1, context=cands)\n",
    "    best  = max(outs, key=lambda x: x[\"score\"])\n",
    "    predictions.append({\"id\": squad[i][\"id\"], \"prediction_text\": best[\"answer\"]})\n",
    "\n",
    "references = [{\"id\": squad[i][\"id\"], \"answers\": answers[i]} for i in range(n)]\n",
    "metric     = load_metric(\"squad\")\n",
    "results    = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"Exact Match: {results['exact_match']:.2f}\")\n",
    "print(f\"F1 Score    : {results['f1']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (benim)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
